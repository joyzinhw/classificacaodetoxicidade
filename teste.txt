Antes do SMOTE: [591 237]
Depois do SMOTE: [591 591]

# import pandas as pd
# import numpy as np
# from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import StandardScaler
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.svm import SVC
# from sklearn.neural_network import MLPClassifier
# from sklearn.metrics import (accuracy_score, precision_score, recall_score,
#                             f1_score, roc_auc_score, matthews_corrcoef,
#                             confusion_matrix, classification_report)
# from rdkit import Chem
# from rdkit.Chem import AllChem
# from rdkit.Chem import DataStructs
# import matplotlib.pyplot as plt
# import seaborn as sns

# # 1. Carregar o dataset
# # Supondo que o arquivo dataset_final.csv est치 no mesmo diret칩rio
# df = pd.read_csv('dataset_final.csv')

# # 2. Pr칠-processamento dos dados
# # Converter SMILES em fingerprints (representa칞칚o num칠rica das mol칠culas)
# def smiles_to_fingerprint(smiles, radius=2, n_bits=1024):
#     mol = Chem.MolFromSmiles(smiles)
#     if mol is None:
#         return None
#     fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=n_bits)
#     arr = np.zeros((1,))
#     DataStructs.ConvertToNumpyArray(fp, arr)
#     return arr

# # Aplicar a convers칚o para todas as mol칠culas
# X = np.array([smiles_to_fingerprint(smiles) for smiles in df['SMILES']])
# y = df['label'].values  # Labels bin치rios (0 = n칚o t칩xico, 1 = t칩xico)

# # Remover mol칠culas que n칚o puderam ser convertidas (se houver)
# valid_indices = [i for i, x in enumerate(X) if x is not None]
# X = np.array([X[i] for i in valid_indices])
# y = y[valid_indices]

# # 3. Dividir os dados em conjuntos de treino e teste (80/20)
# X_train, X_test, y_train, y_test = train_test_split(
#     X, y, test_size=0.2, random_state=42, stratify=y)

# # 4. Padronizar os dados
# scaler = StandardScaler()
# X_train = scaler.fit_transform(X_train)
# X_test = scaler.transform(X_test)

# # 5. Treinar modelos de classifica칞칚o
# # Random Forest
# rf = RandomForestClassifier(n_estimators=100, random_state=42)
# rf.fit(X_train, y_train)

# # SVM
# svm = SVC(kernel='rbf', probability=True, random_state=42)
# svm.fit(X_train, y_train)

# # MLP (Rede Neural)
# mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=300, random_state=42)
# mlp.fit(X_train, y_train)

# # 6. Avaliar os modelos
# def evaluate_model(model, X_test, y_test, model_name):
#     y_pred = model.predict(X_test)
#     y_proba = model.predict_proba(X_test)[:, 1]

#     print(f"\nAvalia칞칚o do modelo {model_name}:")
#     print("="*50)
#     print(f"Acur치cia: {accuracy_score(y_test, y_pred):.4f}")
#     print(f"Precis칚o: {precision_score(y_test, y_pred):.4f}")
#     print(f"Recall: {recall_score(y_test, y_pred):.4f}")
#     print(f"F1-score: {f1_score(y_test, y_pred):.4f}")
#     print(f"AUC-ROC: {roc_auc_score(y_test, y_proba):.4f}")
#     print(f"Matthews Correlation Coefficient: {matthews_corrcoef(y_test, y_pred):.4f}")

#     # Matriz de confus칚o
#     cm = confusion_matrix(y_test, y_pred)
#     plt.figure(figsize=(6, 4))
#     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
#                 xticklabels=['N칚o T칩xico', 'T칩xico'],
#                 yticklabels=['N칚o T칩xico', 'T칩xico'])
#     plt.title(f'Matriz de Confus칚o - {model_name}')
#     plt.ylabel('Verdadeiro')
#     plt.xlabel('Predito')
#     plt.show()

#     # Relat칩rio de classifica칞칚o
#     print("\nRelat칩rio de Classifica칞칚o:")
#     print(classification_report(y_test, y_pred, target_names=['N칚o T칩xico', 'T칩xico']))

# # Avaliar todos os modelos
# models = {'Random Forest': rf, 'SVM': svm, 'MLP': mlp}
# for name, model in models.items():
#     evaluate_model(model, X_test, y_test, name)

# # 7. An치lise de import칙ncia de caracter칤sticas (apenas para Random Forest)
# if hasattr(rf, 'feature_importances_'):
#     importances = rf.feature_importances_
#     indices = np.argsort(importances)[::-1][:20]  # Top 20 caracter칤sticas

#     plt.figure(figsize=(10, 6))
#     plt.title("Top 20 Caracter칤sticas Mais Importantes")
#     plt.bar(range(20), importances[indices[:20]], align='center')
#     plt.xticks(range(20), indices[:20])
#     plt.xlabel('칈ndice da Caracter칤stica')
#     plt.ylabel('Import칙ncia')
#     plt.show()

# Instalar pacotes necess치rios (caso n칚o tenha)
# pip install imbalanced-learn xgboost lightgbm catboost

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                             f1_score, roc_auc_score, matthews_corrcoef,
                             confusion_matrix, classification_report)
from imblearn.over_sampling import SMOTE
from rdkit import Chem
from rdkit.Chem import AllChem, DataStructs
import matplotlib.pyplot as plt
import seaborn as sns

from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

# ===========================
# 1. Carregar Dataset
# ===========================
df = pd.read_csv('dataset_final.csv')

# Converter SMILES em fingerprints
def smiles_to_fingerprint(smiles, radius=2, n_bits=1024):
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return None
    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=n_bits)
    arr = np.zeros((1,))
    DataStructs.ConvertToNumpyArray(fp, arr)
    return arr

X = np.array([smiles_to_fingerprint(sm) for sm in df['SMILES']])
y = df['label'].values

# Remover entradas inv치lidas
valid_indices = [i for i, x in enumerate(X) if x is not None]
X = np.array([X[i] for i in valid_indices])
y = y[valid_indices]

# ===========================
# 2. Split e Escalonamento
# ===========================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# ===========================
# 3. Balanceamento com SMOTE
# ===========================
sm = SMOTE(random_state=42)
X_train_bal, y_train_bal = sm.fit_resample(X_train, y_train)

print("Antes do SMOTE:", np.bincount(y_train))
print("Depois do SMOTE:", np.bincount(y_train_bal))

# ===========================
# 4. Modelos + Otimiza칞칚o
# ===========================
# Definir modelos com hiperpar칙metros b치sicos
models = {
    'Random Forest': RandomForestClassifier(random_state=42),
    'SVM': SVC(probability=True, random_state=42),
    'MLP': MLPClassifier(max_iter=500, random_state=42),
    'XGBoost': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),
    'LightGBM': LGBMClassifier(random_state=42),
    'CatBoost': CatBoostClassifier(verbose=0, random_state=42)
}

# Hiperpar칙metros para busca
param_grid = {
    'Random Forest': {
        'n_estimators': [100, 200],
        'max_depth': [None, 10, 20],
        'min_samples_split': [2, 5]
    },
    'SVM': {
        'C': [0.1, 1, 10],
        'gamma': ['scale', 'auto'],
        'kernel': ['rbf', 'poly']
    },
    'MLP': {
        'hidden_layer_sizes': [(100,), (100,50)],
        'alpha': [0.0001, 0.001],
        'activation': ['relu', 'tanh']
    },
    'XGBoost': {
        'n_estimators': [100, 200],
        'max_depth': [3, 6],
        'learning_rate': [0.01, 0.1]
    },
    'LightGBM': {
        'n_estimators': [100, 200],
        'max_depth': [-1, 10],
        'learning_rate': [0.01, 0.1]
    },
    'CatBoost': {
        'iterations': [100, 200],
        'depth': [4, 6],
        'learning_rate': [0.01, 0.1]
    }
}

# ===========================
# 5. Fun칞칚o de Avalia칞칚o
# ===========================
def evaluate_model(model, X_test, y_test, name):
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]

    print(f"\n=== {name} ===")
    print(f"Acur치cia: {accuracy_score(y_test, y_pred):.4f}")
    print(f"Precis칚o: {precision_score(y_test, y_pred):.4f}")
    print(f"Recall: {recall_score(y_test, y_pred):.4f}")
    print(f"F1-score: {f1_score(y_test, y_pred):.4f}")
    print(f"AUC-ROC: {roc_auc_score(y_test, y_proba):.4f}")
    print(f"MCC: {matthews_corrcoef(y_test, y_pred):.4f}")
    print("\nRelat칩rio de Classifica칞칚o:")
    print(classification_report(y_test, y_pred, target_names=['N칚o T칩xico', 'T칩xico']))

    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['N칚o T칩xico', 'T칩xico'],
                yticklabels=['N칚o T칩xico', 'T칩xico'])
    plt.title(f'Matriz de Confus칚o - {name}')
    plt.ylabel('Verdadeiro')
    plt.xlabel('Predito')
    plt.show()

# ===========================
# 6. Treinar e Avaliar
# ===========================
for name, model in models.items():
    print(f'\n游댌 Otimizando {name}...')
    search = RandomizedSearchCV(model, param_distributions=param_grid[name],
                                n_iter=5, cv=3, scoring='f1', random_state=42, n_jobs=-1)
    search.fit(X_train_bal, y_train_bal)
    best_model = search.best_estimator_
    evaluate_model(best_model, X_test, y_test, name)
